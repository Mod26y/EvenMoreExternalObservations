Original Article: https://www.darkreading.com/cyber-risk/open-source-ai-models-pose-risks-of-malicious-code-vulnerabilities

**1) What Happened:**  
Malicious actors are increasingly exploiting open-source repositories like Hugging Face to post AI models laden with harmful or vulnerable code. This is evidenced by recent incidents where malicious code embedded within AI models bypassed Hugging Face’s automated security checks, particularly using Pickle files. The attacks serve as proof-of-concept, exposing the capacity of threat actors to manipulate repositories using novel techniques such as “NullifAI” to evade detection. Companies rapidly adopting open-source AI in internal projects are thereby at risk of inadvertently integrating compromised components, heightening their susceptibility to security breaches.

**2) Why It Matters:**  
This situation underscores a substantial risk in the emerging domain of AI-driven systems, where undetected malicious codes can lead to severe vulnerabilities such as unauthorized code execution and backdoor installations. The wide adoption of open-source AI models by companies heightens the urgency for robust security mechanisms to mitigate risks inherent in these dependencies. Moreover, reliance on repository security checks is inadequate, pointing to a need for more rigorous internal security assessments. This issue extends beyond technical vulnerabilities to include legal risks, as discrepancies in AI licensing can inadvertently result in IP violations.

**3) What Actions Should Be Taken:**  
Organizations should enhance their security protocols by implementing rigorous internal reviews and code audits for any open-source AI models integrated into their systems. Transitioning from insecure formats like Pickle to safer alternatives like Safetensors is advisable. Furthermore, a comprehensive understanding of AI licensing terms is crucial to prevent potential infringements. Adopting a holistic risk management approach akin to evaluating open-source software dependencies is essential. This includes assessing the model’s source, update frequency, community engagement, and associated security risks. Organizations can benefit from fostering collaboration with the cybersecurity community to continuously refine AI model safety testing techniques.