Original Article: https://www.darkreading.com/threat-intelligence/employees-sensitive-data-genai-prompts

The article "Employees Enter Sensitive Data Into GenAI Prompts Far Too Often" explores the issue of employees unintentionally sharing sensitive data when using generative AI (GenAI) tools. Analysis by Harmonic researchers reveals that 8.5% of GenAI prompts include sensitive data, such as customer details, employee information, legal and financial records, and security information. This data, once inputted, can become part of the AI's learning dataset, raising concerns about security risks if the data is retrieved or misused in the future. This largely stems from the ease with which these tools can be used for seemingly benign tasks.

The exposure of sensitive data through GenAI tools is significant because it poses severe risks of personal data breaches and corporate espionage. Customer data, making up the majority of the leakages, compromises privacy and can damage an organizationâ€™s reputation. The leakage of employee data tracks closely behind, risking workplace confidentiality and operational integrity. Furthermore, legal, financial, and security information exposure could result in severe business risks, and data breaches could have legal repercussions. The improper handling of sensitive data in the GenAI pipeline may lead to substantial financial and reputational repercussions for organizations.

Organizations should consider implementing strong AI governance policies to balance the benefits and risks of GenAI. They should track input in GenAI tools, categorize sensitive data, and ensure that employees use secure, paid GenAI plans designed to be more data-protective. Employee training on data handling and GenAI risks is crucial. Enforcing workflows for GenAI use can mitigate risks by guiding employees on responsible use of these tools. These steps would help avoid unintended data exposure while harnessing the competitive advantages of AI technology in enhancing efficiency and productivity.