Original Article: https://www.bleepingcomputer.com/news/artificial-intelligence/openai-tests-watermarking-for-chatgpt-4o-image-generation-model/

**1) What happened:**

OpenAI is testing watermarking for images generated by the ChatGPT 4o Image Generation model. This model is part of ChatGPT and has recently been made available to free users, whereas it was previously confined to paid subscribers. The watermarking feature is speculated to primarily affect free users' images, preserving the option for ChatGPT Plus subscribers to generate unmarked images. This development addresses the growing use of the model to create recognizable art styles, such as Studio Ghibli art, and hints at OpenAI's efforts to manage the use and potential misuse of its technology.

**2) Why it matters:**

The implementation of watermarking signals OpenAI's response to the ethical and copyright challenges surrounding AI-generated content, especially as the model gains wider accessibility. By watermarking images produced by free users, OpenAI aims to maintain a level of accountability and traceability for AI-generated works. This move potentially reduces the risk of misuse in art forgery and plagiarism while also addressing concerns from artists and studios about unauthorized uses of their styles. It illustrates proactive steps to mitigate the ethical implications of powerful AI tools.

**3) What actions should be taken as a result of this information:**

Organizations adopting AI technology should consider implementing similar measures to ensure their use aligns with legal and ethical standards. Stakeholders involved with AI-generated content should advocate for transparency and accountability in generative models. Rigorous monitoring and the establishment of usage policies can prevent misuse. Ensuring that their cybersecurity infrastructure is equipped to handle new threats associated with AI misuse can further safeguard an organization, and for developers, exploring OpenAI's upcoming ImageGen API can offer opportunities to innovate responsibly.