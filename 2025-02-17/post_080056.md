Original Article: https://www.darkreading.com/cyber-risk/open-source-ai-models-pose-risks-of-malicious-code-vulnerabilities

1) The article describes how attackers are planting malicious AI model projects into open-source repositories like Hugging Face by exploiting security gaps. This includes the use of the "NullifAI" technique to bypass site checks. Notably, malicious files traveling through the Pickle format enable arbitrary code execution. Despite safety mechanisms in place, models with embedded malicious code have still been tagged as safe, reflecting the ineffective detection of such anomalies.

2) This situation matters because many organizations rely on open-source AI models for their projects, increasing their vulnerability to cyber threats. The presence of malicious code in AI models poses significant risks, including code execution and data security breaches. As AI adoption grows, these threats underscore the critical need for better security measures in managing AI supply chains. Ignoring these vulnerabilities could lead to compromised systems and unauthorized access, causing serious operational disruptions and data breaches.

3) Organizations should implement enhanced security protocols to scrutinize AI models from open-source repositories. Transitioning from Pickle to more secure formats like Safetensors is advisable. Additionally, they should conduct thorough due diligence on licensing agreements to safeguard against intellectual property violations. Regular audits of AI supply chains, coupled with adopting advanced detection methods, can help mitigate these risks. Understanding the ecosystemâ€™s components and managing them like any other open-source dependencies will further enhance security and governance in AI deployments.