Original Article: https://www.darkreading.com/cyberattacks-data-breaches/slack-ai-patches-bug-that-let-attackers-steal-data-from-private-channels

### 1) What Happened

Salesforce's Slack Technologies patched a vulnerability in its Slack AI feature that could have been exploited by attackers to steal data from private channels or conduct phishing operations. The flaw, discovered by security firm PromptArmor, was a prompt injection vulnerability, facilitating an attacker's ability to manipulate the large language model (LLM) used by Slack AI. This loophole existed because the LLM could not distinguish between legitimate user instructions and malicious ones. Notably, the vulnerability extended to uploaded documents and Google Drive files ingested by Slack AI, broadening the risk surface.

### 2) Why It Matters

The importance of this issue stems from the widespread use of Slack as a collaboration platform where sensitive business data is often shared. Data breaches and phishing attacks via Slack could lead to significant information exposure and compromise internal communications, potentially resulting in severe financial and reputational damage. Moreover, the flaw raises broader concerns about the safety and robustness of AI tools, highlighting their susceptibility to manipulation. Given the increasing adoption of AI in business processes, ensuring these tools' security and ethical handling of data is crucial for maintaining trust and safeguarding sensitive information.

### 3) What Actions Should Be Taken

Organizations using Slack should consider tightening their Slack AI settings to limit document ingestion capabilities, thereby reducing access to sensitive information by potential threat actors. Regularly updating software to incorporate security patches is essential. Conducting security training focused on recognizing phishing attempts and understanding potential AI vulnerabilities can also enhance preparedness. Furthermore, continuously monitoring Slack channels for suspicious activity and employing additional security measures, such as multi-factor authentication and strict access controls, can mitigate risks. Beyond immediate actions, investing in ongoing AI security evaluations will help preempt and address emerging vulnerabilities.