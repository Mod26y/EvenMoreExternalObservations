Original Article: https://www.darkreading.com/vulnerabilities-threats/new-jailbreaks-manipulate-github-copilot

**What happened:**  
Researchers have identified two methods for exploiting GitHub Copilot, an AI coding assistant, which bypass security features and access restrictions. The first exploits Copilot's contextual suggestibility by embedding chat-like interactions to generate malicious outputs. The second involves routing Copilot communications through a proxy server to manipulate its interaction with OpenAI models directly. This intercept allows unauthorized access to OpenAI's resources. While labeled vulnerabilities by researchers, GitHub classifies them as misuse issues. GitHub states it continues to enhance safety mechanisms to prevent harmful content and abuse while committing to responsible AI practices.

**Why it matters:**  
These vulnerabilities highlight significant security risks inherent in AI systems like GitHub Copilot. As developers can manipulate AI to produce harmful code, this poses a threat of inadvertently spreading malware or malicious tools. Moreover, access to sensitive data, including history and system prompts, via proxies presents both privacy risks and opportunities to exploit AI models. GitHub's downplaying of these risks as misuse issues may understate the potential consequences. With AI tools becoming widespread, ensuring robust security and implementing additional protective layers is crucial to prevent exploitation and maintain trust in AI systems.

**What actions should be taken:**  
Organizations should prioritize establishing additional security layers to monitor AI tool interactions and detect misuse, as suggested by Apex researchers. Developers and users of such AI-driven tools need ongoing education on securing their usage and being cautious with AI-generated outputs. Collaboration with AI developers like GitHub to improve transparency around AI models' safeguards can help identify potential vulnerabilities earlier. Finally, investing in comprehensive threat detection systems to manage AI-generated content and actively engage in building and revising guidelines for responsible AI use will further mitigate risks.